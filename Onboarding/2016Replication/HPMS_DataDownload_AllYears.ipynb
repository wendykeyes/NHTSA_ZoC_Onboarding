{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wendy's Replicated Notebook and Process to Download HPMS data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procedure:\n",
    "    1. need list of states and districts to include in the analysis\n",
    "    2. generate list of URLs to download from HPMS website\n",
    "    3. run loop to download data\n",
    "        correct any filename problems\n",
    "    4. quality check the shapefiles to ensure polylines\n",
    "    5. join shapefiles to geodatabase\n",
    "    6. zip HPMS file geodatabase for each year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Modules\n",
    "import us\n",
    "import arcpy\n",
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "import shutil\n",
    "import io\n",
    "import datetime\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User inputs include a directory to store downloaded files, and the range of years to include in the study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Input the file directory where you want to store the HPMS data C:\\Users\\wen10109\\Documents\\Projects\\NHTSA\\Onboarding\\2016Replication\\DataDownloads\\HPMS\\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the first year in your study range?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " 2011\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the last year in your study range?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " 2017\n"
     ]
    }
   ],
   "source": [
    "# Define workspace to hold new files\n",
    "workspace = input(\"Input the file directory where you want to store the HPMS data\")\n",
    "years = []\n",
    "print(\"What is the first year in your study range?\")\n",
    "year_st = int(input())\n",
    "print(\"What is the last year in your study range?\")\n",
    "year_end = int(input())\n",
    "for year in range(year_st,year_end+1):\n",
    "    years.append(year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workspace already exists\n"
     ]
    }
   ],
   "source": [
    "# Create the workspace file geodatabase\n",
    "if not os.path.exists(os.path.join(workspace, \"hpms_workspace.gdb\")):\n",
    "    os.makedirs(os.path.join(workspace,\"hpms_workspace.gdb\"))\n",
    "else: \n",
    "    print(\"Workspace already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the lists to aid in downloading the shapefiles\n",
    "In future versions, a list of states can be included with checkboxes for subsets of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of states to include in the analysis\n",
    "states_list = [state.name.lower().replace(\" \",\"\") for state in us.states.STATES]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a list of URLs for the download files\n",
    "# First, the base of the URL\n",
    "base_download_url = r\"https://www.fhwa.dot.gov/policyinformation/hpms/shapefiles/\"\n",
    "\n",
    "# Create dataframe of URLs for each state_year download\n",
    "\n",
    "rows = []\n",
    "for state in states_list:\n",
    "    for year in years:\n",
    "        if state == 'missouri' and year == 2015:\n",
    "            rows.append([state, year, f'{base_download_url}/{state}{year}t.zip'])\n",
    "        elif state == 'districtofcolumbia':\n",
    "            rows.append([state, year, f'{base_download_url}/district{year}.zip'])\n",
    "        else:\n",
    "            rows.append([state, year, f'{base_download_url}/{state}{year}.zip'])\n",
    "urls = pd.DataFrame(rows, columns = ['state','year','URL'])\n",
    "\n",
    "# Create dataframe of shapefile extensions for each state_year download\n",
    "rows = []\n",
    "Sy_years = [2011, 2012, 2017]\n",
    "sy_years = [2013]\n",
    "S_years = [2014, 2016]\n",
    "S_Sections_years = [2015] \n",
    "\n",
    "for state in states_list:\n",
    "    for year in years:\n",
    "        if year in Sy_years:\n",
    "            rows.append([state, year, f'{state.capitalize()}{year}.shp'])\n",
    "        elif year in sy_years:\n",
    "            rows.append([state, year, f'{state.lower()}{year}.shp'])\n",
    "        elif year in S_years:\n",
    "            rows.append([state, year, f'{state.capitalize()}.shp'])\n",
    "        elif year in S_Sections_years:\n",
    "            if state == 'missouri' and year == 2015:\n",
    "                rows.append([state, year, f'{state.capitalize()}_Sectionst.shp'])\n",
    "            else:\n",
    "                rows.append([state, year, f'{state.capitalize()}_Sections.shp'])\n",
    "                         \n",
    "shapefiles = pd.DataFrame(rows, columns = ['state', 'year', 'extension'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GDB already exists\n"
     ]
    }
   ],
   "source": [
    "# Create the path folder for the geodatabase\n",
    "gdb = os.path.join(workspace,\"hpms_workspace.gdb\")\n",
    "gdb\n",
    "if not os.path.exists(gdb):\n",
    "    os.makedirs(gdb)\n",
    "else:\n",
    "    print(\"GDB already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the shapefiles from HPMS website\n",
    "https://www.fhwa.dot.gov/policyinformation/hpms/shapefiles.cfm/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wen10109\\Documents\\Projects\\NHTSA\\Onboarding\\2016Replication\\DataDownloads\\HPMS\\hpms_workspace.gdb\\2011\\alabama already exists...skipping to next state\n",
      "C:\\Users\\wen10109\\Documents\\Projects\\NHTSA\\Onboarding\\2016Replication\\DataDownloads\\HPMS\\hpms_workspace.gdb\\2011\\alabama\\Alabama2011.shp already exists...skipping to next state\n",
      "C:\\Users\\wen10109\\Documents\\Projects\\NHTSA\\Onboarding\\2016Replication\\DataDownloads\\HPMS\\hpms_workspace.gdb\\2011\\alaska already exists...skipping to next state\n",
      "C:\\Users\\wen10109\\Documents\\Projects\\NHTSA\\Onboarding\\2016Replication\\DataDownloads\\HPMS\\hpms_workspace.gdb\\2011\\alaska\\Alaska2011.shp already exists...skipping to next state\n",
      "C:\\Users\\wen10109\\Documents\\Projects\\NHTSA\\Onboarding\\2016Replication\\DataDownloads\\HPMS\\hpms_workspace.gdb\\2011\\arizona already exists...skipping to next state\n",
      "C:\\Users\\wen10109\\Documents\\Projects\\NHTSA\\Onboarding\\2016Replication\\DataDownloads\\HPMS\\hpms_workspace.gdb\\2011\\arizona\\Arizona2011.shp already exists...skipping to next state\n",
      "C:\\Users\\wen10109\\Documents\\Projects\\NHTSA\\Onboarding\\2016Replication\\DataDownloads\\HPMS\\hpms_workspace.gdb\\2011\\arkansas already exists...skipping to next state\n",
      "Requesting https://www.fhwa.dot.gov/policyinformation/hpms/shapefiles//arkansas2011.zip from website\n",
      "Extracting https://www.fhwa.dot.gov/policyinformation/hpms/shapefiles//arkansas2011.zip\n"
     ]
    },
    {
     "ename": "BadZipFile",
     "evalue": "File is not a zip file",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBadZipFile\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-7ba9fcad394f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Extracting {file_url}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m             \u001b[0mzipfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0myr_state_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m             \u001b[0mshapefiles_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshapefile_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'{shapefile_path} added to list'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\ESRI\\conda\\envs\\NHTSA\\lib\\zipfile.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, file, mode, compression, allowZip64)\u001b[0m\n\u001b[0;32m   1106\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1107\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1108\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_RealGetContents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1109\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'w'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'x'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1110\u001b[0m                 \u001b[1;31m# set the modified flag so central directory gets written\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\ESRI\\conda\\envs\\NHTSA\\lib\\zipfile.py\u001b[0m in \u001b[0;36m_RealGetContents\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1173\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mBadZipFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"File is not a zip file\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1174\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mendrec\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1175\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mBadZipFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"File is not a zip file\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1176\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1177\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mendrec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mBadZipFile\u001b[0m: File is not a zip file"
     ]
    }
   ],
   "source": [
    "# Iteration Loop to download each state_year shapefile (skip if it already exists), extract contents, and place in a directory that can be deleted or archived later, add name of directories to list\n",
    "shapefiles_list = []\n",
    "\n",
    "for year in years:\n",
    "    for state in states_list:\n",
    "        file_url = urls['URL'].loc[(urls['state']== state) & (urls['year']==year)].get_values()[0]\n",
    "        yr_state_path = f'{os.path.join(gdb, str(year), state)}'\n",
    "        path = shapefiles['extension'].loc[(shapefiles['state']==state) & (shapefiles['year']==year)].get_values()[0]\n",
    "        shapefile_path = f'{yr_state_path}\\\\{path}'\n",
    "\n",
    "        if os.path.exists(yr_state_path):\n",
    "            print(f'{yr_state_path} already exists...skipping to next state')\n",
    "        else:\n",
    "            os.makedirs(yr_state_path)\n",
    "            print(f'Directory created for {yr_state_path}')\n",
    "\n",
    "        if os.path.exists(shapefile_path):\n",
    "            print(f'{shapefile_path} already exists...skipping to next state')\n",
    "            shapefiles_list.append(shapefile_path)\n",
    "        else:\n",
    "            print(f'Requesting {file_url} from website')\n",
    "            response = requests.get(file_url)\n",
    "            print(f'Extracting {file_url}')\n",
    "            if response\n",
    "            zipfile.ZipFile(io.BytesIO(response.content)).extractall(path=yr_state_path)\n",
    "            shapefiles_list.append(shapefile_path)\n",
    "            print(f'{shapefile_path} added to list')\n",
    "\n",
    "print('Downloads complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-QC shapefile count: 21\n",
      "C:\\Users\\wen10109\\Documents\\Projects\\NHTSA\\Onboarding\\2016Replication\\DataDownloads\\HPMS\\hpms_workspace.gdb\\2011\\alabama\\Alabama2011.shp is valid\n",
      "C:\\Users\\wen10109\\Documents\\Projects\\NHTSA\\Onboarding\\2016Replication\\DataDownloads\\HPMS\\hpms_workspace.gdb\\2011\\alaska\\Alaska2011.shp is valid\n",
      "C:\\Users\\wen10109\\Documents\\Projects\\NHTSA\\Onboarding\\2016Replication\\DataDownloads\\HPMS\\hpms_workspace.gdb\\2011\\arizona\\Arizona2011.shp is valid\n",
      "C:\\Users\\wen10109\\Documents\\Projects\\NHTSA\\Onboarding\\2016Replication\\DataDownloads\\HPMS\\hpms_workspace.gdb\\2012\\alabama\\Alabama2012.shp is valid\n",
      "C:\\Users\\wen10109\\Documents\\Projects\\NHTSA\\Onboarding\\2016Replication\\DataDownloads\\HPMS\\hpms_workspace.gdb\\2012\\alaska\\Alaska2012.shp is valid\n",
      "C:\\Users\\wen10109\\Documents\\Projects\\NHTSA\\Onboarding\\2016Replication\\DataDownloads\\HPMS\\hpms_workspace.gdb\\2012\\arizona\\Arizona2012.shp is valid\n",
      "C:\\Users\\wen10109\\Documents\\Projects\\NHTSA\\Onboarding\\2016Replication\\DataDownloads\\HPMS\\hpms_workspace.gdb\\2013\\alabama\\alabama2013.shp is valid\n",
      "C:\\Users\\wen10109\\Documents\\Projects\\NHTSA\\Onboarding\\2016Replication\\DataDownloads\\HPMS\\hpms_workspace.gdb\\2013\\alaska\\alaska2013.shp is valid\n",
      "C:\\Users\\wen10109\\Documents\\Projects\\NHTSA\\Onboarding\\2016Replication\\DataDownloads\\HPMS\\hpms_workspace.gdb\\2013\\arizona\\arizona2013.shp is valid\n",
      "C:\\Users\\wen10109\\Documents\\Projects\\NHTSA\\Onboarding\\2016Replication\\DataDownloads\\HPMS\\hpms_workspace.gdb\\2014\\alabama\\Alabama.shp is valid\n",
      "C:\\Users\\wen10109\\Documents\\Projects\\NHTSA\\Onboarding\\2016Replication\\DataDownloads\\HPMS\\hpms_workspace.gdb\\2014\\alaska\\Alaska.shp is valid\n",
      "C:\\Users\\wen10109\\Documents\\Projects\\NHTSA\\Onboarding\\2016Replication\\DataDownloads\\HPMS\\hpms_workspace.gdb\\2014\\arizona\\Arizona.shp is valid\n",
      "C:\\Users\\wen10109\\Documents\\Projects\\NHTSA\\Onboarding\\2016Replication\\DataDownloads\\HPMS\\hpms_workspace.gdb\\2015\\alabama\\Alabama_Sections.shp is valid\n",
      "C:\\Users\\wen10109\\Documents\\Projects\\NHTSA\\Onboarding\\2016Replication\\DataDownloads\\HPMS\\hpms_workspace.gdb\\2015\\alaska\\Alaska_Sections.shp is valid\n",
      "C:\\Users\\wen10109\\Documents\\Projects\\NHTSA\\Onboarding\\2016Replication\\DataDownloads\\HPMS\\hpms_workspace.gdb\\2015\\arizona\\Arizona_Sections.shp is valid\n",
      "C:\\Users\\wen10109\\Documents\\Projects\\NHTSA\\Onboarding\\2016Replication\\DataDownloads\\HPMS\\hpms_workspace.gdb\\2016\\alabama\\Alabama.shp is valid\n",
      "C:\\Users\\wen10109\\Documents\\Projects\\NHTSA\\Onboarding\\2016Replication\\DataDownloads\\HPMS\\hpms_workspace.gdb\\2016\\alaska\\Alaska.shp is valid\n",
      "C:\\Users\\wen10109\\Documents\\Projects\\NHTSA\\Onboarding\\2016Replication\\DataDownloads\\HPMS\\hpms_workspace.gdb\\2016\\arizona\\Arizona.shp is valid\n",
      "C:\\Users\\wen10109\\Documents\\Projects\\NHTSA\\Onboarding\\2016Replication\\DataDownloads\\HPMS\\hpms_workspace.gdb\\2017\\alabama\\Alabama2017.shp is valid\n",
      "C:\\Users\\wen10109\\Documents\\Projects\\NHTSA\\Onboarding\\2016Replication\\DataDownloads\\HPMS\\hpms_workspace.gdb\\2017\\alaska\\Alaska2017.shp is valid\n",
      "C:\\Users\\wen10109\\Documents\\Projects\\NHTSA\\Onboarding\\2016Replication\\DataDownloads\\HPMS\\hpms_workspace.gdb\\2017\\arizona\\Arizona2017.shp is valid\n",
      "Pose-QC shapefile count: 21\n"
     ]
    }
   ],
   "source": [
    "# QC Check the shapefile for valid geometry and shape type before merging. Remove any shapefiles without Polyline shape types.\n",
    "print(f'Pre-QC shapefile count: {len(shapefiles_list)}')\n",
    "\n",
    "for shapefile in shapefiles_list:\n",
    "    if arcpy.Describe(shapefile_path).shapeType != \"Polyline\":\n",
    "        print(shapefile, arcpy.Describe(shapefile_path).shapeType)\n",
    "        shapefiles_list.remove(shapefile)\n",
    "    else:\n",
    "        print(f'{shapefile} is valid')\n",
    "\n",
    "print(f'Pose-QC shapefile count: {len(shapefiles_list)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\wen10109\\\\Documents\\\\Projects\\\\NHTSA\\\\Onboarding\\\\2016Replication\\\\DataDownloads\\\\HPMS\\\\hpms_workspace.gdb'"
      ]
     },
     "execution_count": 425,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all data files into zipped files (one for each year)\n",
    "for year in years:\n",
    "    output_path = os.path.join(workspace, f'HPMS_Nat_{year}')\n",
    "    if not os.path.exists(output_path):\n",
    "        os.path.makedirs(output_path)\n",
    "        print(f'Directory for {year} output created')\n",
    "    output_hpms_year = arcpy.Merge_management(inputs=shapefiles_list, output=output_path).getOutput(0)\n",
    "    output_hpms_year\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that files are zipped and merged into one geodatabase, the temporary folders can either be deleted to save disk space, or archived for storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " Y\n"
     ]
    }
   ],
   "source": [
    "# Prompt user to decide whether to keep archived files (they may take up storage space)\n",
    "archive = \"Do you want to delete or archive the downloaded files? Type Y to delete, otherwise, files will be archived.\"\n",
    "if input().lower() == \"y\":\n",
    "    if arcpy.Exists(os.path.join(workspace, \"hpms_workspace.gdb\")):\n",
    "        shutil.rmtree(path=gdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.exists(os.path.join(workspace,\"hpms_workspace.gdb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
